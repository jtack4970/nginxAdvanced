<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>NGINX - Advanced Load Balancing</title>

		<meta name="description" content="Learn and practice with NGINX Plus!">
		<meta name="author" content="James Tacker">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/nginx.css" id="theme">

        <!--favicon-->
        <link rel="shortcut icon" href="assets/images/nginxfavicon.ico" type="image/x-icon" />

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

	  <div class="reveal">

        <style>
          .reveal .slides { text-align: left; }
          .reveal .slides h1 { text-align: center; }
          .reveal .slides h2 { text-align: center; }
          .reveal .slides h3 { text-align: center; font-variant: none; text-transform: none;}
        </style>
		<!-- Any section element inside of this container is displayed as a slide -->
		<img src="assets/images/nginxlogo.png" style="border:0; width:100px; height:100px; background:none; position:absolute; left:0; top:0;">
        <img id="lab_pic" src="assets/images/Laboratory3.png" style="visibility:hidden; border:0; width:200px; height:200px; background:none; position:absolute; right:0; top:0;">

        <div class="footer">
          <font size="1">© Copyright 2017 by ServiceRocket, Inc | Confidential | Prepared for NGINX Inc.</font>
        </div>

	        <div class="slides">

              <section data-background="rgb(20, 149, 62)">
                <h1>Adv. Load Balancing</h1>

                <p style="text-align:center">
	              <small><i>Flawless Application Delivery</i></small>
                </p>
              </section>

              <section>
 <!--IF JACOB IS TEACHING-->

		<!--<h3>Trainer Intro</h3>

                    <div style="float:left;width:50%;" class="centered">
                      <p>
                      <strong>Jacob Teal</strong>
                      <p>Technology Consultant</p>
                      <p>B.A. Computational Mathematics</p>
                      <p><a href="mailto:jacob.teal@servicerocket.com">jacob.teal@servicerocket.com</a></p>
					  </p>
                    </div>

                    <div style="float:right;width:40%;padding-right:0px;">
                      <img src="assets/images/starfish.png" style="border:0;background:none; left:0; top:0;">
                    </div>
				</section>

              <section>
                <h3>Course Administration</h3>
                <ul>
                  <li>This session is ~3 hours long</li>
                  <ul><li>10 min. breaks every hour</li></ul>
                  <li>Slides, demonstrations and exercises</li>
                  <li>PDF copy of the materials is available</li>
                  <li>Ask questions at any time</li>
                  <ul><li>Use the chat window</li></ul>
                </ul>
              </section>-->
 <!--IF James IS TEACHING-->
           
              
					<h3>Trainer Intro</h3>

                    <div style="float:left;width:50%;" class="centered">
                      <p>
                      <strong>James Tacker</strong>
                      <p>Technology Consultant & Content Developer</p>
                      <p>Previous Training Work:</p>
                      <ul>
                      	<li>Sauce Labs</li>
                      	<li>New Relic</li>
                      	<li>Salesforce</li>
                      	<li>Atlassian</li>
                      </ul>
                      <p><a href="mailto:james.tacker@servicerocket.com">james.tacker@servicerocket.com</a></p>

					  </p>
                    </div>

                    <div style="float:right;width:40%;padding-right:0px;">
                      <img src="assets/images/Picture1.png" style="border:0;background:none; left:0; top:0;">
                    </div>
				</section>

	      <section>
		<h3>Prerequisites/Expectations</h3>
		<ul>
		  <li>Sysadmin, DevOps, Solution Architect</li>
		  <li>Completed NGINX Core</li>
		  <li>Some familiarity with Linux</li>
		  <li>Text Editor: Vim, Vi, Emacs etc.</li>
		  <li>Solid understanding of Network topologies</li>
		</ul>
		<aside class="notes">
			<p>This course is designed for those who want to take their understanding and skills with nginx to the next level.</p>
<p>This course is the next step in your roadmap to become a certified NGINX solutions architect.
This course assumes you have basic Linux command line knowledge as well as how to use a text editor like vim or nano.a
For those of you using Windows, you’ll want to a Linux OS on a virtual machine of your choosing.</p>

		</aside>
	      </section>
	      
              <section>
                <h3>The Training Environment</h3>
		
                <ul>
                  <li>AWS EC2 Instances</li>
                  <li>Ubuntu</li>
                  <li>NGINX Plus</li>
                  <li>Wordpress</li>
                  <li>Tomcat 7</li>
                </ul>
		
                <aside class="notes">
                 
		  
                </aside>
              </section>

	      <section>
		<h3>Log Into AWS</h3>
		<p>If you haven't done so already, please take the time to SSH into your EC2 Instances (Windows users use <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html" target="_blank">PuTTY</a>).</p>
		<p>Check your email for the login credentials, check your spam folder!</p>
		<pre><code class="linux" data-trim contenteditable>
                        ssh student&#60;number&#62;&#64;&#60;ec2-server-hostname&#62;
                  </code></pre>
	      </section>
	      
	      <section>
		<h3>Course Administration</h3>
		<ul>
			<li>Course Duration: 4 hours</li>
			<li>Ask questions at any time!</li>
		</ul>
	      </section>
	      
              <section>
                <h3>Agenda</h3>

                <div style="text-align:left;">
                    <img src="assets/images/NGINXAdvancedAgenda1.png" style="border:0;background:none; left:0; top:0;">
                </div>
		
		<aside class="notes">
		<p>So for this first day we're going to continue our understanding of how to lock down NGINX to a greater degree than what we learned in NGINX Core, then we will explore the web server use case and learn about the configuration file, then we will usetup a proxy server, as well as learn about logging. Eventually we will setup our site to use ssl, and then we will round out the day learning about variables</p>

		</aside>
              </section>

              <section data-background="rgb(20, 149, 62)">
                <h2>Load Balancing Review</h2>
              </section>

                <section>
                  <h3>Module Objectives</h3>
                  <p>This module enables you to:</p>
                  <ul>
                    <li>Configuration Overview</li>
                    <li>Hardware Migration</li>
                    <li>Error Pages</li>
		  </ul>
                  <aside class="notes"></aside>
                </section>

		<section>
		  <h3>Value of NGINX in DevOps Chain</h3>
		  <img src="assets/images/DevOpsChain.png" style="border:none; background:none; width:100%">
		  <aside class="notes">
		    <p>NGINX is an open source reverse proxy server. For those who don’t know, a reverse proxy server is a proxy server, sitting behind a firewall in a private network, that directs client requests to appropriate backend servers.</p>
<p>Common uses cases for a reverse proxy server are: Load Balancer, or as I like to call it a web traffic cop, that sits in front of something like apache and distributes client request across a group of servers based on the load they’re handling. HTTP Cache, reverse proxy servers can cache incoming common requests which will speed up the flow of traffic between your clients and servers by reducing the amount of redundant tasks that your backend servers need to manage. Web Server: NGINX can also take over the responsibilities of a web server and host websites that are accessible by the internet.</p>
		  </aside>
		</section>

		<section>
			<h3>Load Balancing Components</h3>
			<ul>
				<li>Selection Algorithm</li>
				<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">upstream</span></pre></li>
				<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">proxy_pass</span></pre></li>
				<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">health_check</span></pre></li>
			</ul>
			<aside class="notes"></aside>
		</section>

		<section>
			<h3>Load Balancing Configuration</h3>
			<pre><code class="linux" data-trim contenteditable>
				upstream myServers {
    server localhost:8080;
    server localhost:8081;
    server localhost:8082;
}

server {
    listen 80;
    root /usr/share/nginx/html;

    location / {
        proxy_pass http://myServers;
    }
}
			</code></pre>
			<aside class="notes"></aside>
		</section>

		<section data-state="lab">
		  <h3>Lab 1.1: Configure a New Upstream</h3>
		  <ol>
		    <li>Create a configuration file called <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">main.conf</pre> in <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">/etc/nginx/conf.d</span></pre> with a <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">server</span></pre> that listens on <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">80</span></pre></li>
		    <li> Add three servers in the <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">upstream</span></pre> block (ask your instructor for the backend urls)</li>	
		    <li>Create a <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">location</span></pre> prefix to <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">proxy_pass</span></pre> to your <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">upstream</span></pre> group.</li>
		    <li>Define an <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">error_log</span></pre> with a level of <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">info</span></pre> and an <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">access_log</span></pre> with a level of <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">combined</span></pre></li>
		   	<li>Save and reload NGINX</li>
		   	<li>Test in a local browser (refresh multiple times)</li>
		    <li>Read the <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">access_log</span></pre> to see destination of request</li>
		  </ol>
		  <aside class ="notes">
		  	<ol>
		  	<li>Open /etc/nginx/conf.d and rename the default.conf to default.conf.bak</li>
<li>Create a file called main.conf in the conf.d folder in /etc/nginx</li>
<li>Add two location blocks that match requests for /application1 and /application2</li>
<li>Add a third location block that matches requests for /images</li>
<li>Override the root directive in the /images location block to specify the folder where your images are stored (i.e /data/images)</li>
<li>Open your browser to your server’s URL. What can you observe?</li>
<li>Now open (server)/application1/app1.html. What can you observe?</li>
<li>Now hit (server)/images/logo.png What can you observe?</li>

</aside>

		</section>

		<section>
			<section>
			<h3>Migrating from Hardware</h3>
			<ul>
				<li>No need to <a href="https://www.nginx.com/blog/nginx-load-balance-deployment-models/" target="_blank">"rip and replace"</a></li>
				<li>Can work in parallel with legacy hardware</li>
				<li>Terminology Differences</li>
				<ul>
					<li>NSIP, SNIP = Host IP</li>
					<li>VIP = Virtual IP address</li>
					<li>Monitor = <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">health_check</span></pre></li>
				</ul>
			</li>
		</ul>
			<aside class="notes"></aside>
		</section>

		<section>
			<h3>Deployment Scenario 1</h3>
			<center><p>NGINX does ALL Load Balancing</p></center>
			<center><img src="assets/images/deployment1.png" style="border:0; background:none; width:80%"></center>

			<aside class="notes">
				<p>Clients connect directly to NGINX Plus which then acts as a reverse proxy, load balancing requests to pools of backend servers. This scenario has the benefit of simplicity with just one platform to manage, and can be the end result of a migration process that starts with one of the other deployment scenarios we will discuss next. <p>

				<p>If SSL/TLS is being used, NGINX Plus can offload SSL/TLS processing from the backend servers. This not only frees up resources on the backend servers, but centralizing SSL/TLS processing also increases SSL/TLS session reuse. Creating SSL/TLS sessions is the most CPU‑intensive part of SSL/TLS processing, so increasing session reuse can have a major positive impact on performance.</p>

<p>Both NGINX and NGINX Plus can be used as a cache for static and dynamic content, and NGINX Plus adds the ability to purge items from the cache, especially useful for dynamic content.</p>

<p>NGINX Plus offers additional ADC functions, such as application health checks, session persistence, response rate limiting, bandwidth limiting, connection limiting, and more.</p>

<p>To support high availability (HA) in this scenario requires clustering of the NGINX Plus instances.</p>
			</aside>
		</section>

		<section>
			<h3>Deployment Scenario 2</h3>
			<center><p>NGINX Works in Parallel with Legacy Hardware</p></center>
			<center><img src="assets/images/nginxside3.png" style="border:0; background:none; width:80%"></center>

			<aside class="notes">
<p>NGINX Plus is introduced to load balance new applications in an environment where a legacy hardware appliance continues to load balance existing applications.</p>

<p>This scenario can be applied in a data center where both the hardware load balancers and NGINX Plus reside, or the hardware load balancers might be in a legacy data center while NGINX Plus is deployed in a new data center or a cloud.</p>

<p>The usual reason for deploying NGINX in this way is that a company wants to move to a more modern software‑based platform but does not want to rip and replace all of its legacy hardware load balancers. </p>
			</aside>
		</section>

		<section>
			<h3>Deployment Scenario 3</h3>
			<center><p>NGINX Sits behind Legacy Hardware</p></center>
			<center><img src="assets/images/nginxbehind.png" style="border:0; background:none; width:80%"></center>

			<aside class="notes">

<p>NGINX Plus is added to an environment with a legacy hardware‑based load balancer, but here it sits behind the legacy load balancer. Clients connect to The hardware‑based load balancer accepts client requests and load balances them to a pool of NGINX Plus instances, which load balance them across the group of actual backend servers. In this scenario NGINX Plus performs all Layer‑7 application load balancing and caching.</p>

<p>Because the NGINX Plus instances are being load balanced by the hardware load balancer, HA can be achieved by having the hardware load balancer do health checks on the NGINX Plus instances and stop sending traffic to instances that are down.</p>

<p><strong>disadvantages</strong>: There can be multiple reasons for deploying NGINX Plus in this way. One is because of corporate structure. In a multi‑tenant environment where many internal application teams share a device or set of devices, the hardware load balancers are often owned and managed by the network team. The application teams probably would like access to the load balancers to add application‑specific logic, but the complexity of true multi‑tenancy means that even sophisticated solutions can still not provide complete isolation between one application and another. If the application teams were given free access to shared devices, one team might make configuration changes that negatively impact other teams.</p>

<p>To avoid the potential problems, the network team often retains sole control over the hardware load balancers. The application teams have to submit requests to make any configuration changes. In addition, because of the potential for configuration conflicts between teams, the network team is likely to limit which advanced ADC features are exposed, meaning that application teams can’t take advantage of all the functionally available on the hardware load balancer.</p>

<p>One solution to this problem is to deploy a set of smaller load balancers, such as NGINX Plus, so that each application team can have its own. Completely isolated from one another, the application teams can each take full advantage of all the features they need without risking negative consequences for other teams. It’s not cost effective to give each application team a set of hardware appliances, so this a great use case for a software‑based load balancer like NGINX Plus.</p>

<p>The hardware load balancers remain in place, still owned and managed by the network team, but they no longer have to deal with complex multi‑tenant issues or application logic; their only job is to get the requests to the right NGINX Plus instances where the application logic resides, and NGINX Plus routes the requests to the right backend servers. This provides the network team with the control they need while also enabling the application teams to take full advantage of the ADC functionality.</p>
			</aside>
		</section>
	</section>

		<section>
			<section>
			<h3>Configuration Translations</h3>
			<ul>
				<li>Request Redirect: <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">return</span></pre></li>
				<li>Request Rewrite: <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">rewrite</span></pre></li>
				<li>Response Rewrite: <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">sub_filter</span></pre></li>
				<li>Searching Files: <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">try_files</span></pre></li>
			</ul>
			<aside class="notes"></aside>
		</section>

		<section>
			<h3>Request Redirect</h3>
			<pre><code class="linux" data-trim contenteditable>
			#F5 iRule 
when HTTP_REQUEST { 
	HTTP::redirect "https://[getfield [HTTP::host] ":" 1][HTTP::uri]" 
}
----------------------------------------------------------------------
#NGINX
location / { 
	return 301 https://$host$request_uri; 
}
</code></pre>
			<aside class="notes">
For complicated redirects consider using embedded scripting languages like perl or lua

			</aside>
		</section>

		<section>
			<h3>Request Rewrite</h3>
<pre><code class="linux" data-trim contenteditable>
			#F5 iRule
when HTTP_REQUEST { 
	if {[string tolower [HTTP::uri]] matches_regex {^/music/([a-z]+)/([a-z]+)/?$} }  { 
		set myuri [string tolower [HTTP::uri]] 
		HTTP::uri [regsub {^/music/([a-z]+)/([a-z]+)/?$} $myuri "/mp3/\\1-\\2.mp3"] 
	}
 }
 -------------------------------------------------------------------------------

#NGINX
location ~*^/music/[a-z]+/[a-z]+/?$ { 
	rewrite ^/music/([a-z]+)/([a-z]+)/?$ /mp3/$1-$2.mp3 break; 
	proxy_pass http://music_backend; 
}
</code></pre>
			<aside class="notes"></aside>
		</section>

		<section>
			<h3>Response Rewrite</h3>
			<pre><code class="linux" data-trim contenteditable>
			#F5 iRule	
when HTTP_RESPONSE { 
	if  {[HTTP::header value Content-Type] contains "text"} { 
		STREAM::expression {@/mp3/@/music/@} 
		STREAM::enable 
	}
 }
--------------------------------------------------------------

#NGINX
location / { 
	sub_filter '/mp3/' '/music/'; 
	proxy_pass http://default_backend; 
}
</code></pre>
			<aside class="notes">
				<p>
Rewriting the response body is often done along with request routing, to change links in the response body to reflect the new URI structure created by the request routing. It can also be used to make other changes to the response body before it’s sent to the client.
To rewrite HTTP responses with NGINX Plus, use the sub_filter directive. It takes two parameters: the string for NGINX Plus to search for and replace, and the replacement string.</p>

<p>This example searches through the response body for links containing the /mp3/ directory element and replace it with /music/ </p>


			</aside>
		</section>
	</section>

		<section>
			<section>
			<h3>Searching For Files</h3>
			<p><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">if</span></pre> directive is bad practice</p>
			<p><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">try_files</span></pre> directive is a better choice</p>

			<aside class="notes"></aside>
		</section>

		<section>
			<h3><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 60px;">if</span></pre> Directive</h3>
			<ul>
				<li>Can cause NGINX to SIGSEGV</li>
				<li>Essentially creates a nested <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">location</span></pre> block that has to run on every request
</li>
				<li>Only 100% safe use cases:
<ul>
	<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">return...;</span></pre></li>
	<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">rewrite ... last/permanent;</span></pre></li>
</ul>
</li>
			</ul>
			<pre><code class="linux" data-trim contenteditable>
				if ($request_method = POST ) { 
    return 405; 
}
---------------------------------------------------

if ($args ~ post=140){ 
	rewrite ^ http://example.com/ permanent; 
}
			</code></pre>
			<aside class="notes">

<p>With the If Directive the specified nested condition is evaluated. If it’s true, the directives specified inside those braces are executed, and that initial request is assigned to the configuration details inside the if directive.</p>

<p>It’s worth mentioning here that configurations inside the if directives are inherited from the previous configuration level, so you can basically thinkg of an if directive as a nested location</p>

<p>This is where the problems begin, if the 'if' directive has problems when used in a location context, in some cases it doesn’t do what you expect but rather something completely different instead.</p> 
<p>In some cases it even segfaults. (i.e. segmentation fault (aka segfault)</p>
 This is common condition with systems under significant load and will cause programs to crash; they are often associated with a file named core . Segfaults are caused by a program trying to read or write an illegal memory location) It’s generally a good idea to avoid this if possible.</p>

<p>There are cases where you simply cannot avoid using an if, for example, if you need to test a variable which has no equivalent directive, like in this example which tests the condition of a Post request and returns a internal redirect code, or if an argument equals a certain post value and then the url can be rewritten as a permanent redirect.</p>

<p>It is important to note that the behaviour of if is not inconsistent, given two identical requests it will not randomly fail on one and work on the other, with proper testing and understanding ifs ‘’‘can’‘’ be used. It’s recommend to thoroughly read through the two links posted in this slide but if you’re in doubt one should use the return, rewrite, or try_files directives instead.</p>

<p>What we should do instead is use the try_files directive</p>

			</aside>
		</section>

		<section>
			<h3><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 60px;">try_files</span></pre> Directive</h3>
			<ul>
				<li>NGINX checks for the existence of files and/or directories in order</li>
				<li>Commonly uses the <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">$uri</span></pre> variable</li>
				<li>If no file or directory exists, NGINX performs an <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">internal</span></pre> redirect</li>
			</ul>
<pre><code class="linux" data-trim contenteditable>
location / { 
	try_files $uri $uri/ @proxy; 
}

location @proxy { 
	proxy_pass http://backend/index.php;
}
</code></pre>
			<aside class="notes">
<p>Like the return and rewrite directives, the try_files directive is placed in a server or locationblock. As parameters, it takes a list of one or more files and directories and a final URI:</p>

<p>try_files file … uri;</p>

<p>NGINX checks for the existence of the files and directories in order (constructing the full path to each file from the settings of the root and alias directives), and serves the first one it finds. To indicate a directory, add a slash at the end of the element name. If none of the files or directories exist, NGINX performs an internal redirect to the URI defined by the final element (uri).</p>
<p>For the try_files directive to work, you also need to define a location block that captures the internal redirect, as shown in the following example. The final element can be a named location, indicated by an initial at‑sign (@).</p>

<p>The try_files directive commonly uses the $uri variable, which represents the part of the URL after the domain name.</p>

<p>In the following example, NGINX serves a default GIF file if the file requested by the client doesn’t exist. When the client requests (for example) http://www.domain.com/images/image1.gif, NGINX first looks for image1.gif in the local directory specified by the root or alias directive that applies to the location (not shown in the snippet). If image1.gif doesn’t exist, NGINX looks for image1.gif/, and if that doesn’t exist, it redirects to /images/default.gif. That value exactly matches the second location directive, so processing stops and NGINX serves that file and marks it to be cached for 30 seconds.</p>


			</aside>
		</section>
	</section>

	<section>
			<h3><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 60px;">error_page</span></pre> Directive</h3>
			<ul>
				<li>Create and reference custom error pages</li>
<li>Best practices:
	<ul>
		<li>Set <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">root</span></pre> for <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">error_page</span></pre></li>
		<li>Separate messages for each code or range</li>
	</ul></li>
</ul>
<pre><code class="linux" data-trim contenteditable>
	error_page 404 /404.html;
location = /404.html {
    root /usr/share/nginx/html;
}

error_page 500 502 503 504 /50x.html;
location /50x.html {
    root /usr/share/nginx/html;
}

	</code></pre>
			<aside class="notes"></aside>
		</section>

		<section data-state="lab">
		  <h3>Lab 1.2: I am Error</h3>
		  <ol>
		    <li>In <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">main.conf</pre> force the <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">server</span></pre>  to only accept <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">ipv6</span></pre> traffic</li>
		    <pre><code class="linux" data-trim contenteditable>
		    	listen [::]:80 default_server ipv6only=on;
		    </code></pre>
		    <li>Write two <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">error_page</span></pre> directives for <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">404</span></pre> and <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">50x</span></pre> codes</li>
		    <li>Append a custom page location for each directive</li>
		    <pre><code class="linux" data-trim contenteditable>
		    	error_page 404 /custom_404.html;
error_page 500 502 503 504 /custom_50x.html;
		    </code></pre>
		    <li>Add a prefix called <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">/test</span></pre> along with the following line:</li>
		    <pre><code class="linux" data-trim contenteditable>
		    	fastcgi_pass unix:/null/path;
		    </code></pre>
		    <li>Reload NGINX, test your ec2-url and custom pages</li>
		  </ol>
		  <aside class ="notes">
		  	<pre><code class="linux" data-trim contenteditable>
		  	upstream myServers {
    zone http_backend 64k;

    server server:8080 route=backend1;
    server server:8080 route=backend2;
    server server:8080 route=backend3;
}

server {
    listen 80;
    listen [::]:80 default_server ipv6only=on;
    error_log /var/log/nginx/main.error.log info;
    access_log /var/log/nginx/main.access.log combined;

    error_page 404 /custom_404.html;
    error_page 500 502 503 504 /custom_50x.html;

    location / {
        proxy_pass http://myServers/;
    }

    location = /custom_404.html {
        internal;
    }

    location = /custom_50x.html {
        internal;
    }

    location /test {
        fastcgi_pass unix:/null/path;
    }
    </code></pre>
</aside>

		</section>
	      
<!--Adv. Security Module-->
              <section data-background="rgb(20, 149, 62)">
                <h2>Advanced Security</h2>
              </section>

                <section>
                  <h3>Module Objectives</h3>
                  <p>This module enables you to:</p>
                  <ul>
                    <li>Signature Strength</li>
                    <li>Cipher Strength</li>
                    <li>A+ Score on SSL Labs.com</li>
		  </ul>
                  <aside class="notes"></aside>
                </section>

<section>
	<h3>General Infrastructure Security</h3>
	<ul>
		<li>Turn off server_tokens</li>
		<li>Turn off corresponding backend engine headers
			<ul>
				<li>X-Powered-By</li>
			</ul>
		</li>
		<li>Change client side error pages</li>
		<li>Encrypt ALL THE THINGS!!!</li>
		<li>Test on <a href="https://www.ssllabs.com/ssltest/" target="_blank">SSL Labs.com</a></li>
	</ul>
	<aside class="notes"></aside>
</section>

<section>
	<h3>Getting a Perfect SSL Score</h3>
	<ul>
		<li>Verify cert/chain in order and from trusted authority</li>
		<li>Use strong signature algorithm</li>
		<li>Use the latest protocol support</li>
		<li>Generate strong key signature</li>
		<li>Use preferred ciphers</li>
	</ul>
	<aside class="notes"></aside>
</section>

<section>
	<h3>Configure HTTPS</h3>
	<ul>
		<li>Enable <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">ssl</span></pre> on <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">listen</span></pre> directive</li>
		<li>Specify <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">ssl_certifcate</span></pre> and <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">ssl_certificate_key</span></pre></li>
	</ul>
	<pre><code class="linux" data-trim contenteditable>
		server  {
    listen  443 ssl;
    root /data;

    ssl_certificate /etc/nginx/ssl/nginx.crt;
    ssl_certificate_key /etc/nginx/ssl/nginx.key;

}

	</code></pre>
	<aside class="notes"></aside>
</section>

<section>
	<h3><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 60px;">ssl_dhparam</span></pre></h3>
	<ul>
		<li>Specifies a file with DH parameters for <a href="https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange" target="_blank">DHE</a> ciphers</li>
		<li>Aids in Forward Secrecy</li>
		<li>Make them strong, 4096 rather than 2048</li>
	</ul>
<pre><code class="linux" data-trim contenteditable>
server  {
    listen  443 ssl;
    root /data;

    ssl_certificate /etc/nginx/ssl/nginx.crt;
    ssl_certificate_key /etc/nginx/ssl/nginx.key;
    ssl_dhparam ssl/dhparam.pem; 
}

</code></pre>

	<aside class="notes">
<p>By default, Nginx will use the default DHE (Ephemeral Diffie-Hellman) paramaters provided by openssl. This uses a weak key that gets lower scores. The best thing to do is build your own. You can create a 2048 bit key, but it's better to toss a 4096 at NGINX. It will take a very long time to create, so there should be an existing one in your ssl directory.</p>

<p>After you've build the key, go ahead and reference it using the ssl_dhparam directive</p>
	</aside>
</section>

<section>
	<h3><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 60px;">ssl_protocols</span></pre></h3>
	<ul>
		<li>Specifies which protocols are enabled</li>
		<li>Best Practice: Only support TLS</li>
		<li>Best Practice: Only support latest for higher rating</li>
	</ul>
	<pre><code class="linux" data-trim contenteditable>
server  {
    listen  443 ssl;
    root /data;

    ssl_certificate /etc/nginx/ssl/nginx.crt;
    ssl_certificate_key /etc/nginx/ssl/nginx.key;
    ssl_dhparam ssl/dhparam.pem; 
    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
}

</code></pre>
	<aside class="notes">
<p>The best place to start with securing your website is protocol support. It's a given that you shouldn't be using SSLv2. It should also be a given that you should no longer be using SSLv3. The recent POODLE vulnerability pretty much put the final knife in it.</p>
<p>This means you should only support the TLS protocols.</p>
<p>If you want a "perfect" score however, you should only use TLSv1.2</p>

	</aside>
</section>

<section>
	<h3><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 60px;">ssl_ciphers</span></pre></h3>
	<ul>
		<li>Specifies a list based on business needs</li>
		<li>Moving target; ciphers constantly change, new security threats arise daily</li>
		<li>Configure NGINX to force client to accept preferred order of ciphers</li>
	</ul>
	<pre><code class="linux" data-trim contenteditable>
		server  {
    listen  443 ssl;
    root /data;

    ssl_certificate /etc/nginx/ssl/nginx.crt;
    ssl_certificate_key /etc/nginx/ssl/nginx.key;
    ssl_dhparam ssl/dhparam.pem; 
    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
    ssl_ciphers AES256+EECDH:AES256+EDH:!aNULL;
    ssl_prefer_server_ciphers on;
}
	</code></pre>
	<aside class="notes">
<p>This one especially is ever changing. What's best today, may not be so hot tomorrow. Here, you'll have a battle between high security and compatibility. Keeping with the topic of this module, we should care about security only.</p>
	</aside>
</section>

<section>
	<h3><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 60px;">ssl_stapling</span></pre></h3>
	<ul>
		<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">ssl_stapling</span></pre> ”staples” an OCSP response</li>
<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">ssl_stapling_verify</span></pre>	verifies OCSP responses</li>
<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">ssl_trusted_certificate</span></pre> required</li>
</ul>
<pre><code class="linux" data-trim contenteditable>
	server  {
    listen  443 ssl;
    root /data;

    ssl_certificate /etc/nginx/ssl/nginx.crt;
    ssl_certificate_key /etc/nginx/ssl/nginx.key;
    ssl_dhparam ssl/dhparam.pem; 
    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
    ssl_ciphers AES256+EECDH:AES256+EDH:!aNULL;
    ssl_prefer_server_ciphers on;
    ssl_stapling on;
    ssl_stapling verify on;
}
</code></pre>
	<aside class="notes">
		<p>SSL Stapling doesn't exactly make you any more secure, but it does help the client significantly. In short, you help the client by telling them they can use your server for OCSP information for your domain instead of letting the browser make the request to an often unreliable outside resource.</p> <p>In more technical terms what's happening here is:</p>
<ul>
		<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">ssl_stapling</span></pre>  directive will ”staples” an OCSP response to the initial TLS handshake, thus relieving the client from having to contact the CA</li>
<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">ssl_stapling_verify</span></pre>	Enables verification of OCSP responses by the server rather than the client</li>
<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">ssl_trusted_certificate</span></pre> must be used in order for stapling to work. This ticket must contain the intermediate & root certificates (in that order from top to bottom). This ticket is usually indicated by the .pem extension</li>
</ul>
	</aside>
</section>


<section>
	<h3><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 60px;">ssl_session</span></pre></h3>
	<ul>
		<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">ssl_session_cache</span></pre></li>
		<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">ssl_session_timeout</span></pre></li>
		<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">ssl_handshake_timeout</span></pre></li>
	</ul>
	<aside class="notes">
<ul>
	<li>ssl_session_cache shared:name:size; Size is specified in bytes or megabytes. Can be specified in the http or server context</li>
<li>ssl_session_timeout. The session cache can be shared between workers and will timeout after 5 minutes by default</li>
<li>ssl_handshake_timeout. Speeds up handshake operation, not recommend setting this value too low or too high.</li>
</ul>

	</aside>
</section>

<section>
	<h3>HSTS</h3>
	<ul>
		<li>Forces browsers to communicate over <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">https</span></pre></li>
		<li>Other Security Precautions:</li>
		<ul>
		<li>No-Content-Sniffing</li>
		<li>No content displaying in iFrames</li>
	</ul></li>
	</ul>
	<pre><code class="linux" data-trim contenteditable>
		add_header Strict-Transport-Security max-age=63072000; 
add_header X-Content-Type-Options nosniff;
add_header X-Frame-Options DENY; 
	</code></pre>
	<aside class="notes"></aside>
</section>

<section>
	<h3>"Dual Stack" RSA and ECC</h3>
	<ul>
		<li>ECC is 3x faster than RSA</li>
		<li>Include both pairs to support both</li>
	</ul>
	<pre><code class="linux" data-trim contenteditable>
		server { 
    listen 443 ssl; 
    server_name example.com;

    ssl_certificate example.com.rsa.crt;    ssl_certificate_key example.com.rsa.key;
    ssl_certificate example.com.ecdsa.crt;  ssl_certificate_key example.com.ecdsa.key; 
}

	</code></pre>	
	<aside class="notes"></aside>
</section>

<section>
	<h3>Elliptic Curves</h3>
	<p>NGINX doesn’t have a good default for elliptic curves, so specify manually e.g.</p>
	<p><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">ssl_ecdh_curve secp384r1;</span></pre></p>

	<aside class="notes"></aside>
</section>
		
		<section data-state="lab">
		  <h3>Lab 2.1: Let's Encrypt</h3>
		  <ol>
		    <li>Add the certbot repositories to your apt utility, and update dependencies</li>
		    <pre><code class="linux" data-trim contenteditable>
		    	sudo add-apt-repository pps:certbot/certbot
		    	sudo apt-get update
		    	sudo apt-get install certbot
		    </code></pre>
		    <li>Obtain a standalone certificate</li>
		    <pre><code class="linux" data-trim contenteditable>
		    	$ certbot certonly --standalone -d ec2-url.com -d www.ec2-url.com
		    </code></pre>
		  </ol>
		  <p></p>
		  <div style="text-align:center;"><small>Documentation: <a href="https://certbot.eff.org/#ubuntuxenial-nginx" target="_blank">Certbot Installation</a></small></div>
		  <aside class ="notes">
		  	<p>Instructions: <a href "https://certbot.eff.org/#ubuntuxenial-nginx" target="_blank"> Link</a></p>

</aside>

		</section>

		<section data-state="lab">
		  <h3>Lab 2.2: Configure SSL Parameters</h3>
		  <ol>
		    <li>Add the following to a new file called <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">ssl.params.conf</span></pre></li>
		    <pre><code class="linux" data-trim contenteditable>
		    	ssl_certificate /etc/letsencrypt/live/&#60;ec2-url&#62;/fullchain.pem; 
ssl_certificate_key /etc/letsencrypt/live/&#60;ec2-url&#62;/privkey.pem;
ssl_dhparam /etc/nginx/ssl/dhparam.pem;
ssl_trusted_certificate /etc/letsencrypt/live/&#60;ec2-url&#62;/cert.pem;
ssl_protocols TLSv1.2;
ssl_ciphers "AES256+EECDH:AES256+EDH:!aNULL";
ssl_prefer_server_ciphers on;
ssl_ecdh_curve secp384r1;
ssl_session_cache shared:SSL:10m;
ssl_session_timeout 10m;
ssl_session_tickets off;
ssl_stapling on;
ssl_stapling_verify on;
add_header Strict-Transport-Security "max-age=63072000; includeSubdomains";
add_header X-Frame-Options DENY;
add_header X-Content-Type-Options nosniff;

		    </code></pre>
		    <li>Change the certificate/key details to your certbot values.</li>
		  </ol>
		  <aside class ="notes">
		  	<ol>
		  	</ol>
</aside>
		</section>

		<section data-state="lab">
		  <h3>Lab 2.3: Configure HTTPS</h3>
		  <ol>
		  	<li>Save and open <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">main.conf</span></pre></li>
		    <li>Add a new <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">return</span></pre> directive in the <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">server</span></pre> context that redirects all traffic to <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">https</span></pre></li>
		    <pre><code class="linux" data-trim contenteditable>
		    	server {
    listen 80
    listen [::]:80 default_server ipv6only=on;
    return 301 https://$host$request_uri;
}

		    </code></pre>
		    <li>Add another <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">server</span></pre> listening on <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">433</span></pre>, using <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">ssl</span></pre> and includes <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">ssl.params.conf</span></pre></li>
		     <pre><code class="linux" data-trim contenteditable>
		    	server { 
    listen 443 ssl; 
    include /etc/nginx/conf.d/ssl.params.conf;
    }
}
</code></pre>
		  </ol>
		  <aside class ="notes">
		  	<pre><code class="linux" data-trim contenteditable>
		  	server {
    listen 80;
    listen [::]:80 default_server ipv6only=on;
    error_log /var/log/nginx/main.error.log info;
    access_log /var/log/nginx/main.access.log combined;
    return 301 https://$host$request_uri;
    status_zone redirect;
}

server {
    listen 443 ssl;
    root /usr/share/nginx/html;
    error_log /var/log/nginx/upstream.error.log info;
    access_log /var/log/nginx/upstream.access.log combined;

    include /etc/nginx/conf.d/ssl.params.conf;
    ...
}
</code></pre>
</aside>
		</section>

		<section data-state="lab">
		  <h3>Lab 2.4: Test on SSL Labs.com</h3>
		  <div style="float:left;width:50%;padding-right:0px;">
		  <ol>
		  	<li>Save and <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">reload</span></pre> NGINX</li>
		  	<li>Test your site on <a href="https://www.ssllabs.com/ssltest/" target="_blank">SSL Labs.com</a></li>
		    <li>Share your results with the class</li>
		  </ol>
		</div>

		  <div style="float:right;width:40%;padding-right:0px;">
		  	<img src="assets/images/aPlus.png" style="border:0; background:none; width:80%">
		  </div>

		  <aside class ="notes">
		  	<ol>
		  	</ol>
</aside>
		</section>
<!--Next Section-->

		<section data-background="rgb(20, 149, 62)">
                  <h2>TCP/UDP Load Balancing</h2>
                 </section>

		<section>
                  <h3>Module Objectives</h3>
                  <p>This module enables you to:</p>
                  <ul>
                    <li>Explore L7 and L4 differences with NGINX Plus</li>
                    <li>Differentiate between <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">stream</span></pre> and <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">http</span></pre> context</li>
                    <li>Configure logging for TCP/UDP upstream</li>
                    <li>Create Active Health Checks for <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">stream</span></pre> context</li></ul>
                  <aside class="notes"></aside>
        </section>

    <section>
		<h3><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 60px;">http</span></pre> vs. <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 60px;">stream</span></pre></h3>
		 <div style="float:left;width:50%;" class="centered">
		 	<h4><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 40px;">http</span></pre></h4>
		 	<ul>
		 		<li>Parses <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">http</span></pre> request</li>
		 		<li>L7 Layer</li>
		 		<ul>
		 			<li>Header injection</li>
		 			<li>Location routing</li>
		 			<li>SSL termination</li>
		 		</ul>
		 	</ul>
		 </div>
		  <div style="float:left;width:50%;" class="centered">
		  	<h4><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 40px;">stream</span></pre></h4>
		 	<ul>
		 		<li>Raw IP packets</li>
		 		<li>L3/L4 Layer</li>
		 		<ul>
		 			<li>Pass SSL certs</li>
		 			<li>Lower overhead</li>
		 			<li>Network visibility</li>
		 		</ul>
		 	</ul>
		  </div>

		<aside class="notes">
<p>Layer 7: operates at the high-level application layer, which deals with the actual content of each message. HTTP is the predominant Layer 7 protocol for website traffic on the Internet. Layer 7 load balancers route network traffic in a much more sophisticated way than Layer 4 load balancers, particularly applicable to TCP-based traffic such as HTTP. A Layer 7 load balancer terminates the network traffic and reads the message within. It can make a load-balancing decision based on the content of the message (the URL or cookie, for example). It then makes a new TCP connection to the selected upstream server (or reuses an existing one, by means of HTTP keepalives) and writes the request to the server.</p>

<p>Layer 4: operates at the intermediate transport layer, which deals with delivery of messages with no regard to the content of the messages. Transmission Control Protocol (TCP) is the Layer 4 protocol for Hypertext Transfer Protocol (HTTP) traffic on the Internet. Layer 4 load balancers simply forward network packets to and from the upstream server without inspecting the content of the packets. They can make limited routing decisions by inspecting the first few packets in the TCP stream.</p>
		</aside>
	</section>

<section>
		<h3><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 60px;">stream</span></pre> Context</h3>
		<p>Key Differences</p>
		<ul>
			<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">proxy_pass</span></pre> relegated to <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">server</span></pre> context</li>
			<li>Active <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">health_checks</span></pre> work differently than <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">http</span></pre> load balancer</li>
			<li> IP Transparency, <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">proxy_protocol</span></pre>, and Direct Server Return (DSR) instead of <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">proxy_set_header</span></pre></li>
			<li>Logging only available with verison r11 or higher</li>
		</ul>
		<aside class="notes"></aside>
	</section>

<section>
	<section>
		<h3>IP Transparency</h3>
		<h4>The Problem</h4>
		<p>Retain source IP during a TCP (or HTTP) reverse proxy to an application server</p>
		<h4>The Solution</h4>
			<p><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">proxy_bind</span></pre> directive + <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">transparent</span></pre> paramerter</p>
		</ul>
		<pre><code class="linux" data-trim contenteditable>
	stream {		
    server {
        listen 3306;

        location / {
            proxy_bind $remote_addr transparent;
            proxy_pass http://mysql_db_upstream;
        }
    }
}
	</code></pre>
		<aside class="notes">
<p>The intention of IP Transparency is to conceal the presence of the reverse proxy so that the origin server observes that the IP packets originate from the client’s IP address. IP Transparency can be used with TCP‑based and UDP‑based protocols.</p>

<p>In short here you're spoofing the source address of upstream traffic by including the transparent parameter to the proxy_bind directive. Most commonly, you set the source address to that of the remote client:</p>

<p>Now the challenge here is to correctly handle the conneciton details, in other words you need to ensure that response (egress) traffic to the remote client is correctly handled. The response traffic must be routed to the NGINX Plus reverse proxy, and NGINX Plus must terminate the TCP connection. NGINX Plus then sends the response traffic to the remote client using the client TCP connection:</p>
		</aside>
	</section>
	<section>
		<h3>IP Transparency Diagram</h3>
		<center><img src="assets/images/ip-transparency-packet-flow.png" style="border:none; background:none; width:100%"></center>
		<div style="text-align:center;"><small>Documentation: <a href="https://www.nginx.com/blog/ip-transparency-direct-server-return-nginx-plus-transparent-proxy/" target="_blank">IP Transparency</a></small></div>
		<div style="text-align:center;"><small>Documentation: <a href="https://www.nginx.com/blog/ip-transparency-direct-server-return-nginx-plus-transparent-proxy/#upstream-reach-external" target="_blank">Masquerading Traffic</a></small></div>
		<aside class="notes">
<p>So looking at it a little deeper, The IP Transparency mode of operation uses the TPROXY kernel module, which is a standard feature of most modern Linux kernels. You need to make several configuration changes:</p>
<ol>

<li>On the NGINX Plus server, configure the worker processes to run as root, so that they can bind upstream sockets to arbitrary addresses. In the main (top‑level) context in /etc/nginx/nginx.conf, we can include the user directive to set the NGINX Plus worker processes’ ID to root:</li>

<li>On the NGINX Plus server, ensure that each connection originates from the remote client address (Step 2 in the diagram). Add the proxy_bind directive with the transparent parameter to the configuration for the virtual server:</li>

<li>And on the upstream servers we have to configure the routing so that all return traffic is forwarded to NGINX.</li>
<li>Remove any pre‑existing default routes, and check that the routing table looks sensible:</li>
<li>If your upstream servers need to be able to connect to external servers, you need to configure the new NGINX Plus gateway to forward and masquerade traffic </li>

<li>On the NGINX Plus server, we also need to configure the TPROXY kernel module to capture the return packets from the upstream servers and deliver them to NGINX Plus (Step 5 in the diagram).

In the example in the documentation, we run the iptables and ip rule commands to capture all TCP traffic on port 80 from the servers represented by an IP range</li>

</ol>
<p>As you can gather based on this diagram, IP transparency is very complicated, so I recommend setting up a test project to configure this setup, and also make sure you have control over the upstream servers so you can congifure the necessary ip routing capabilities you may need</p>

		</aside>
	</section>

	<section>
		<h3><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 60px;">proxy_protocol</span></pre> Directive</h3>
		<ul>
			<li>Allows NGINX to accept client information via <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">proxy_protocol</span></pre> from proxy servers/load balancers</li> 
			<li>Examples origin services:
			<ul>
				<li>HAProxy</li>
				<li>Amazon ELB</li>
				<li>GCE Active LB</li>
			</ul></li>
		</ul>
<pre><code class="linux" data-trim contenteditable>
		stream {
    server {
        listen 12345;
        proxy_pass example.com:12345;
        proxy_protocol on;
    }
}
</code></pre>
<div style="text-align:center;"><small>Documentation: <a href="https://www.nginx.com/resources/admin-guide/proxy-protocol/" target="_blank">Proxy Protocol Guide</a></small></div>
		<aside class="notes">
<p>The PROXY protocol enables NGINX and NGINX Plus to receive client connection information passed through proxy servers and load balancers such as HAproxy and Amazon Elastic Load Balancer (ELB).

The information passed via the PROXY protocol is the client IP address, the proxy server IP address, and both port numbers. Knowing the originating IP address of a client may be useful for setting a particular language for a website, keeping a blacklist of IPs, or simply for logging and statistics purposes.

With the PROXY protocol, NGINX can learn the originating IP address from SSL, HTTP/2, SPDY, WebSocket, and TCP.</p>
		</aside>
	</section>

	<section>
		<h3>DSR</h3>
		<ul>
		<li>Responses (return packets) bypass Load Balancer</li>
		<li>Takes load off of load balancer</li>
		<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">health_checks</span></pre> no longer work</li>
		<li>Requires further configuration (iptables, Router configuration etc.)</li>
		<pre><code class="linux" data-trim contenteditable>
		server {
    listen 53 udp;

    proxy_bind $remote_addr:$remote_port transparent;
    proxy_responses 0;
    # proxy_timeout 1s;
}
</code></pre>
<div style="text-align:center;"><small>Documentation: <a href="https://www.nginx.com/blog/ip-transparency-direct-server-return-nginx-plus-transparent-proxy/#dsr" target="_blank">DSR Guide</a></small></div>

		<aside class="notes">
<p>Direct Server Return (DSR) is an extension of the IP Transparency concept. In DSR, the upstream server receives packets that appear to originate from the remote client, and responds directly to the remote client. The return packets bypass the load balancer completely.</p>

<p>DSR can deliver a small performance benefit because it reduces the load on the load balancer, but it does carry a number of limitations:</p>
<ul>
<li>The load balancer never sees the return packets, so it cannot detect whether the upstream server is responding or has failed.</li>

<li>The load balancer cannot inspect a request beyond the first packet before selecting an upstream, so its ability to make load‑balancing decisions (content‑based routing) is very limited.</li>
<li>The load balancer cannot participate in any form of negotiation or stateful processing, such as SSL/TLS.</li>
<li>Most other application delivery controller (ADC) features are not possible with DSR, such as caching, HTTP multiplexing, and logging.</li>
</ul>

<p>How does it differ from IP transparency?</p>
<ul>
	<li>NGINX Plus must spoof both the remote client IP address and port when sending datagrams to upstream servers (proxy_bind port configuration).</li>
	<li>NGINX Plus must not be configured to expect response datagrams from upstream servers (the proxy_responses 0 directive).</li>
	<li>An additional step is necessary to rewrite the source address of the return datagrams to match the public address of the load balancer.</li>
</ul>
		</aside>
	</section>
</section>

	<section>
		<h3>SSL Server Name Routing</h3>
		<ul>
			<li>The <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">preread</span></pre> feature can inspect incoming SSL/TLS and determine target</li>
			<li>Can also use the <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">map</span></pre> to determine complex routing method</li>
		</ul>
		<pre><code class="linux" data-trim contenteditable>
		stream {
    server {
        listen 443;
        ssl_preread on;
        proxy_pass $ssl_preread_server_name;
    }
}

</code></pre>
		<aside class="notes">
<p>You can now use NGINX Plus’ TCP/UDP load balancer to load balance SSL/TLS connections without decrypting them. This is useful in a secure or high‑traffic environment where you want to forward SSL/TLS‑encrypted connections to a remote server.
With the new SSL server name preread feature, NGINX Plus R11 can inspect each incoming SSL/TLS connection and determine the target domain (such as the Server Name Indication [SNI] value) to which to route the connection.</p>

<p>The SSL server name is provided in the new $ssl_preread_server_name variable. It contains the name of the target host as extracted from the SNI field of the SSL/TLS handshake.</p>

<p>You can use the variable as the argument to the proxy_pass directive or as a field in the virtual server access log. Note that to enable this feature you must include the ssl_preread directive in the configuration, as shown in this example:</p>

		</aside>
	</section>

	<section>
		<h3>Logging for <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 60px;">stream</span></pre></h3>
		<ul>
			<li>Use <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">access_log</span></pre> to inspect data rates, protocols, error conditions, etc.</li>
			<li>Only available in r11</li>
		</ul>
		<pre><code class="linux" data-trim contenteditable>
			log_format tcp_log '$remote_addr [$time_local] ' '$protocol $status $bytes_sent $bytes_received' '$upstream_session_time $upstream_addr $proxy_protocol_addr’;


</code></pre>
		<aside class="notes">
<p>The new Stream Log module in NGINX Plus R11 provides the same kind of access logging for TCP/UDP connections as was available in previous releases for HTTP connections. You can now log each TCP/UDP session processed by the Stream module, inspecting data rates, load‑balancing decisions, error conditions, and so on. This is a vital feature when debugging or auditing TCP or UDP transactions.</p>

<p>The Stream module exposes a large number of variables and all of them can be logged. You can customize the default log format, using variables in the following fashion:</p>

		</aside>
	</section>

	<section>
		<h3>TCP/UDP Considerations</h3>
		<ul>
			<li>Access Control Limits
			<ul>
				<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">allow</span></pre> , <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">deny</span></pre></li>
				<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">proxy_download_rate</span></pre> , <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">proxy_upload_rate</span></pre></li>
				<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">limit_conn</span></pre> , <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">limit_zone</span></pre></li>
			</ul></li>
			<li>Use <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">slow-start</span></pre> to prevent overload</li>
			<li>Use maintenance parameters to handle failover, updates, migrations etc.
				<ul>
					<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">drain</span></pre></li>
					<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">backup</span></pre></li>
					<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">down</span></pre></li>
				</ul></li>
			</ul>
		<aside class="notes"></aside>
	</section>

	<section data-state="lab">
		  <h3>Lab 3.1: Create TCP Upstream</h3>
		  <ol>
		    <li>In the <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">tcp</span></pre> directory, create/open <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">lb.conf</span></pre></li>
		    <li>Create a <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">server</span></pre> that listens on port <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">90</span></pre> and proxies to <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">tcp_backend</span></pre></li>
		    <pre><code class="linux" data-trim contenteditable>
		    	stream {
    upstream tcp_backend {
        zone tcp_upstream 64k;
        server backend1:8080;
        server backend2:8080;
        server backend3:8080;
    }

    server {
        listen 90;
        proxy_pass tcp_backend;
    }
}

		    </code></pre>
		  </ol>
		  <aside class ="notes">
		  	<ol>
		  	</ol>
</aside>
</section>

<section data-state="lab">
		  <h3>Lab 3.2: Create a UDP Upstream</h3>
		  <ol>
		    <li>Create a <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">server</span></pre> that listens on <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">53</span></pre>, and append the <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">udp</span></pre> parameter</li>
		    <li>Use a <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">proxy_pass</span></pre> to proxy to a new <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">upstream udp_backend</span></pre></li>
		     <pre><code class="linux" data-trim contenteditable>
 upstream udp_backend {
        zone udp_upstream 64k;
        server ec-2:53;
        server ec-2:53;
        server ec-2:53;
    }

    server {
        listen 53 udp;
        proxy_pass udp_backend;
    }
		    </code></pre>
		  </ol>
		  <aside class ="notes">
		  	<ol>
		  	</ol>
</aside>
</section>

	<section>
		<h3>TCP/UDP Health Checks</h3>
		<ul>
			<li>Passive health check use <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">health_check</span></pre></li>
			<li>Active health check use parameters:
				<ul>
					<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">interval</span></pre> , <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">passes</span></pre> , <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">fails</span></pre></li>
				</ul></li>
				<li>Sophisticated health check use <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">match</span></pre> block
					<ul>
						<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">send</span></pre>: text string or hexidecimals</li>
						<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">expect</span></pre>: literal string or regex data response</li>
					</ul></li>
		<aside class="notes"></aside>
	</section>


		<section data-state="lab">
		  <h3>Lab 3.3: TCP Health Check</h3>
		  <ol>
		    <li>Configure a passive <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">health_check</span></pre> for <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">udp</span></pre> and <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">tcp</span></pre> upstreams</li>
		    <li>Test using <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">status.html</span></pre></li>
		    <li>Create a <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">match</span></pre> block the uses a <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">GET</span></pre> request to confirm TCP connection</li>
		    <pre><code class="linux" data-trim contenteditable>
 match http {
    send "GET / HTTP/1.0\r\nHost: localhost:8080\r\n\r\n";
    expect ~* "200 OK";
}

    server {
        listen 90;
        health_check interval=10 passes=5 fails=5 match=http;
        proxy_pass tcp_backend;
    }
		    </code></pre>
		  </ol>
		  <aside class ="notes">
		  	<h4>COMPLETE EXAMPLE</h4>
		 <pre><code class="linux" data-trim contenteditable>
stream {

    #Logging only supported in NGINX Version 1.11.5 or higher
    log_format basic " '$remote_addr [$time_local]' '$protocol $status $bytes_sent $bytes_received''$session_time";

    upstream tcp_backend {
        zone tcp_upstream 64k;
        server server:8080;
        server server:8080;
        server server:8080;
    }

    upstream udp_backend {
        zone udp_upstream 64k;
        server server:53;
        server server:53;
        server server:53;
    }

    match http {
        send "GET / HTTP/1.0\r\nHost: localhost:8080\r\n\r\n";
        expect ~* "200 OK";
     }

    server {
        listen 90;
        access_log /var/log/nginx/nginx_tcp_access.log basic buffer=32k;
        error_log /var/log/nginx/nginx_tcp_error.log info;
        health_check interval=10 passes=5 fails=5 match=http;
        proxy_pass tcp_backend;
    }

    server {
        listen 53;
        proxy_pass udp_backend;
    }
}

		 </code></pre>

</aside>
		</section>

<section>
		<h3>MySQL Load Balancing</h3>
		<ul>
			<li>Configure load balancer and make a SQL query to confirm behavior</li>
			<li>Listening port must use MySQL server port (default <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">3306</span></pre>)</li>
			<pre><code class="linux" data-trim contenteditable>
 stream { 
	upstream db { 
		server db1:3306; 
		server db2:3306; 
		server db3:3306; }
	server { 
		listen 3306; 
		proxy_pass db; 
	}
}
		    </code></pre>
		<aside class="notes">
<p>In some situations, this behavior is acceptable. If an application is unlikely to submit conflicting updates in parallel, and the application code can gracefully handle these very infrequent rejected transactions (by returning an error to the user, for example), then it might not be a serious issue.
If this behavior is not acceptable, the simplest solution is to designate a single primary database instance in the upstream server group, by marking the others as backup and down:</p>
		</aside>
	</section>

	<section>
		<h3>Avoding Parrallel DB Updates</h3>
<ol>
	<li><strong>Failover:</strong> <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">db2</span></pre> acts as a <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">backup</span></pre> and <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">db1</span></pre> receives connections to replicate across other nodes</li>
	<li><strong>Silent Partner:</strong> <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">db3</span></pre> is a silent partner to <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">db1</span></pre> and <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">db2</span></pre></li>
	<li><strong>Failure Detection:</strong> <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">proxy_connect_timeout</span></pre> set to low value (<pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">1</span></pre> second or less) to catch early failures</li>
</ol>
<pre><code class="linux" data-trim contenteditable>
upstream db {
	server db1:3306; 
	server db2:3306 backup; 
	server db3:3306 down; 
}

server {
	listen 3306;	
	proxy_pass db;
	proxy_connect_timeout 1s;
}

		    </code></pre>
		<aside class="notes"></aside>
	</section>

	<section>
		<h3>High Availability</h3>
		<ol>
			<li>keepalived</li>
			<li>Virtual Router Redundancy Protocol (VRRP)</li>
			<li>Health Check</li>
		</ol>
		<aside class="notes">
<p>The keepalived open source project provides the keepalive daemon for Linux servers,
 Use the Virtual Router Redundancy Protocol (VRRP) to manage virtual routers (virtual IP addresses), and a health check facility to determine whether a service (for example, a web server, PHP back end, or database server) is up and operational. If a service on a node fails the configured number of health checks, keepalived reassigns the virtual IP address from the master (active) node to the backup (passive) node.</p>

<p>VRRP also ensures that there is a master node at all times. The backup node listens for VRRP specified packets from the master node. If it does not receive an specified packet for a period longer than three times the configured specified interval, the backup node takes over as master and assigns the configured virtual IP addresses to itself.</p>

		</aside>
	</section>

	<section>
		<h3>Basic Active-Passive Steps</h3>
		<ol>
			<li>Install the package and run the setup:
				<pre><code class="linux" data-trim contenteditable>
$ apt-get install nginx-ha-keepalived
$ nginx-ha-setup
</code></pre></li>
<li>Configure the nginx-ha-check script
	<pre><code class="linux" data-trim contenteditable>
vrrp_script chk_nginx_service {
	 script "/usr/libexec/keepalived/nginx-ha-check" 
	interval 3 
	weight 50 
}
</code></pre></li>
</ol>
<p>Admin Guides:</p>
<ul>
<li><a href="https://www.nginx.com/resources/admin-guide/nginx-ha-keepalived/" target="_blank">HA Admin Guide</a> , <a href="https://www.nginx.com/resources/admin-guide/nginx-ha-keepalived/?utm_source=high-availability-in-nginx-plus-r6&utm_medium=blog&utm_campaign=Core+Product#ha_scripts" target="_blank">Control Mastership</a></li>
</ul>

		<aside class="notes">
			<ol>
<li>Install the package</li>
<li>Run the Setup</li>
<li>Configure the nginx-ha-check to control mastership.</li>
<li>So because there is no fencing mechanism in keepalived. If the two nodes in a pair are not aware of each other, each assumes it is the master and assigns the virtual IP address to itself. To prevent this situation, the configuration file defines a script-execution mechanism called chk_nginx_service that runs a script regularly to check whether NGINX Plus is operational, and adjusts the local node’s priority based on the script’s return code. </li>
<li>Code 0 (zero) indicates correct operation, and code 1 (or any nonzero code) indicates an error. In this example the interval defines the rate in seconds in which the VRRP script will run, and the weight here will indicate which node is the master based on return code. So for example here it is set to 50, which means that when the check script succeeds (returning code 0):</li>
<li>The priority of the first node (which has a base priority of 101) is set to 151.</li>
<li>The priority of the second node (which has a base priority of 100) is set to 150.</li>
<li>The first node has higher priority (151 in this case) and becomes master.</li>
<li>For more detailed setup information about VRRP and the keepalived package definitely checkout the admin guide link here, and for more information about configuring mastership there is a blog detailing how to do it, among other things in the second link.</li>
</ol>
		</aside>
	</section>

	<section>
		<h3>Active-Active</h3>
		<p>Use Cases:</p>
		<ul>
<li>DNS Resolution per service</li>
<li>Share IPs between services</li>
<li>Round Robin to map single DNS to multiple IPs</li>
<li>Layer 3 load-balancing device to distribute L3 traffic between IP addresses</li>
</ul>
<p></p>
 <div style="text-align:center;"><small>Documentation: <a href="https://www.nginx.com/resources/admin-guide/nginx-ha-keepalived-nodes/" target="_blank">Admin Guide</a></small>

		<aside class="notes"></aside>
	</section>

	<section data-state="lab">
		  <h3>Lab 4: All Active GCE LB Demo</h3>

		 <center><img src="assets/images/AllActive.png" style="border:0; background:none; width:50%"></center>
		 <div style="text-align:center;"><small>Documentation: <a href="https://www.nginx.com/resources/deployment-guides/all-active-nginx-plus-load-balancing-gce/" target="_blank">Admin Guide</a></small></div>
		  <aside class ="notes">
<ol>
</ol>

</aside>
		</section>

 <!--Next Section-->
		<section data-background="rgb(20, 149, 62)">
                  <h2>Monitoring Application Performance</h2>
                 </section>

         <section>
                  <h3>Module Objectives</h3>
                  <p>This module enables you to:</p>
                  <ul>
                    <li>Deploy NGINX Amplify to monitor upstreams</li>
                    <li>Setup Dashboards and Alerts</li>
                    <li>Perform Configuration Management</li>
		  </ul>
                  <aside class="notes"></aside>
                </section>

<section>
		<h3>Monitoring Tools</h3>
		<p>Examples of Third Party Monitoring Dashboards:</p>
		<ul>
<li>New Relic</li>
<li>Data Dog (really cool shirts though…)</li>
<li>NGINX Amplify</li>
</ul>

		<aside class="notes"></aside>
	</section>

<section>
		<h3>NGINX Amplify Prerequisites</h3>
<ol>
	<li>Latest version of NGINX Plus</li>
	<li>Running supported Linux distro</li>
	<li>Supported Python Version</li>
	<li>Enable the  <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">status</span></pre> module</li>
</ol>

		<aside class="notes"></aside>
	</section>

	<section data-state="lab">
		  <h3>Lab 5.1: Create an Amplify Account</h3>
		<ol>
			<li>Visit the <a href="http://amplify.nginx.com/signup?utm_source=setting-up-nginx-amplify-in-10-minutes&utm_medium=blog&_ga=1.125460169.295710940.1477586862" target="_blank">NGINX Amplify Singup Page</a></li>
			<li> Click "Signup"</li>
			<li>Fill in addtional information in the dialog box</li>
			<li>Click "Next"</li>
		</ol>
		<aside class="notes"></aside>
	</section>

<section data-state="lab">
		  <h3>Lab 5.2: Amplify Agent</h3>
		<ol>
			<li>Download the amplify agent to the system you wish to monitor
				<pre><code class="linux" data-trim contenteditable>
sudo curl -L -O \
\ https://raw.githubusercontent.com/nginxinc/nginx‑amplify‑agent/master/packages/install.sh
</code></pre></li>
			<li>Run <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">install.sh</span></pre> using your <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">API_KEY</span></pre> (found in "New System" tab)
			<pre><code class="linux" data-trim contenteditable>
				# API_KEY=’&#60;your api key&#62;' sh ./install.sh
			</code></pre></li>
			<li>Check Systems tab to see the EC2 instance</li>
		</ol>
		<aside class="notes"></aside>
	</section>

<section>
		<h3>Dashboards</h3>
			<p>Dashboards allow you to report metrics of incoming data, some options include:</p>
			<ul>
<li>Filtering</li>
<li>Summation</li>
<li>Aggregation</li>
</ul>
		<aside class="notes"></aside>
	</section>

<section>
		<h3>Dashboard Example</h3>
		<p>The Dashboard below shows metrics for 401 codes</p>
		<center><img src="assets/images/Dashboards.png" style="border:0; background:none; width:70%"></center>

		<aside class="notes"></aside>
	</section>

<section data-state="lab">
		  <h3>Lab 5.2: Dashboard Graph</h3>
 <ol>
		  <li>On the Dashboards drop‑down menu, click "Create Dashboard"</li>
<li>Click Add Graph </li>
<li>Enter a "Title" for your graph, such as 404 Errors</li>
<li>Choose nginx.http.status.4xx metric from the drop‑down menu</li>
<li>Select an NGINX instance</li>
<li>Click "Apply Filter", then "Select Filter Key"</li>
<li>Select $status (to represent status code)</li>
<li>Type 404 and then select “use 404 as value”</li>
<li>Click Save. This graph will now appear in a new dashboard called 404 Errors</li>
		  </ol>
		
		  <aside class ="notes">
<ol>
</ol>

</aside>
</section>

<section>
		<h3>Alerts</h3>
		<ul>
		<li>Alert messages can inform you when the value of a certain metric goes outside a specified range</li>
<li>NGINX Amplify Alert Limitations:</li>
<li>Alerts based on summaries or averages across systems</li>
<li>Alerts are not filtered by hostname by default</li>
</ul>
		<aside class="notes">
<p>Properly set up, alerts give you peace of mind, because you learn about unexpected server behavior very quickly. In some cases, an alert can tell you about a problem before it becomes visible to users; in other cases, an alert can ensure that, when a problem is becoming visible to users, you’re not the last to know.</p>

<p>Limitations: </p>
<ul>
	<li>Can’t create alerts based on summaries or averages across a set of systems</li>
	<li>Alerts are not filtered by hostname by default, so an alert tells you when any system goes outside the target range you’ve specified. To make an alert host‑specific, specify the hostname when configuring the alert.</li>
</ul>
<p>Every NGINX Amplify account has one predefined alert: you are alerted if NGINX Amplify goes down for more than two minutes. Remove this alert if you don’t need it.</p>
		</aside>
	</section>

	<section>
		<h3>Alert Example</h3>
		<p>This alert sends an email message when the inbound traffic exceeds 1Mbps</p>
		<center><img src="assets/images/alertExample.png" style="border:0; background:none; width:70%"></center>
		<aside class="notes">
<p>When you select a metric, more fields appear where you choose the factors to compare the metric against, such as a period of time and a threshold value. This alert, triggers when the inbound traffic exceeds 1 Mbps for more than 2 minutes.</p>
<p>You can optionally, specify one or more of your Systems which the alert applies to. If you don’t specify one system, the alert applies to each of your systems. Then specify the email address or email group where the message is sent to</p>
		</aside>
	</section>

<section data-state="lab">
		  <h3>Lab 5.3: Create an Alert</h3>
 <ol>
<li>On the Alerts page, select "Add New Alert"</li>
<li>In the new dialog box, select a metric from the drop‑down menu or type in a text string</li>
<li>Based on your metric more fields will appear, for this exercise choose the <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">nginx.http.status.4xx</span></pre> metric</li>
<li>Choose a threshold of 2 minutes</li>
<li>Choose the System you which to apply the alert</li>
<li>Enter your email address to receive the notification and click "Create Alert"</li>
<li>Test <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">ec2-server/123.html</span></pre> and refresh the webpage enough times to exceed the threshold. An alert message should’ve been sent to your email.</li>

		  </ol>
		
		  <aside class ="notes">
<ol>
</ol>

</aside>
</section>

<section>
		<h3>Configuration Management</h3>
		<p>After installation, NGINX Amplify will:</p>
		<ol>
<li>Perform analysis of NGINX instance and provide status information</li>
<li>Breakdown configuration and provide recommendations to improve the performance and reliability of applications</li>
</ol>

		<aside class="notes">
<p>When you install the agent software on a server, NGINX Amplify analyzes your configuration and generates a detailed report. The report includes status information and configuration advice:</p>
<ul>
<li>Status information. The current state of your NGINX instance, including the NGINX version, an overview of your configuration (including modules), security advisories, virtual servers, and SSL/TLS information.</li>
<li>Static analysis. A breakdown of your configuration that provides recommendations to improve the performance, security, and reliability of your NGINX‑delivered applications.</li>
</ul>


		</aside>
	</section>
<section>

		<h3>Reporting</h3>
		<p>To view NGINX Amplify recommendations, use the Reports tab</p>
		<center><img src="assets/images/reportExample.png" style="border:0; background:none; width:70%"></center>
		<aside class="notes"></aside>
	</section>

 <!--Next Section-->
 <section data-background="rgb(20, 149, 62)">
     <h2>API Gateway</h2>
 </section>

 <section>
    <h3>Module Objectives</h3>
       <p>This module enables you to:</p>
          <ul>
    <li>Explore NGINX API Gateway Topologies</li>
    <li>Explore Microservices Reference Architecture</li>
		  </ul>
    <aside class="notes"></aside>
    </section>

	<section>
<h3>Monolith to Microservices</h3>
<div>
  <div style="float:left;width:50%;" class="centered"><center><img class="fragment" data-fragment-index="0" src="assets/images/monolith.png" style="border:0; background:none; width:100%; position:relative;"></center>
  <center><p class="fragment" data-fragment-index="0">
  	Monolithic Architecture</p></center>
  </div>
 
  <div style="float:right;width:50%;" class="centered"><center><img class="fragment" data-fragment-index="1" src="assets/images/monolith2.png" style="border:0; background:none; width:80.6%; position:relative;"></center>
  	<center><p class="fragment fade-in" data-fragment-index="1">
  Microservices Architecture
  </p></center>
</div>
  </section>

	<section>
		<h3>Service Discovery</h3>
		<div style="float:left;width:50%;" class="centered">
		<ul>
			<li>Services need to know locations of each other</li>
			<li>Registries work in differenty ways</li>
			<li>Register and read information</li>
		</ul>
	</div>
	<div style="float:right;width:50%;" class="centered">
		<center><img src="assets/images/serviceDiscovery1.png" style="border:0; background:none; width:79%"></center>
	</div>
	<aside class="notes"></aside>
</section>

<section>
		<h3>High Quality LB</h3>
		<div style="float:left;width:50%;" class="centered">
		<ul>
			<li>Precise distribution of traffic to services</li>
			<li>Developer Configurable</li>
		</ul>
	</div>
	<div style="float:right;width:45%;" class="centered">
		<center><img src="assets/images/highQualityLB.png" style="border:0; background:none; width:80%"></center>
	</div>
	<aside class="notes">
<ul>

	<li>Once you know where the services are, you need to distribute traffic to them</li>
<li>Load balancing is simple in its dumbest form</li>
<li>Complicated in the more sophisticated formats</li>
<li>And variable as you connect to different types of serivces</li>

</ul>
	</aside>
</section>


	<section>
		<h3>Secure and Fast</h3>
		<ul>
			<li>Encryption at the transmission layer</li>
			<li>SSL handshake slows down communication</li>
			<li>Encryption is CPU intensive</li>
		</ul>
		
		<aside class="notes">
<p>This is perhaps the most daunting aspect of microservice design:</p>
<ul>
<li>What was in memory communication is now going over the network which is an order of magnitude slower in the best scenario</li>
<li>What was securely contained data on a single system is now being flung across the network in a text based format that is very easy to read </li>
<li>If you add encryption at the transmission layer, you introduce significant overhead in terms of connection rates, CPU usage, you name it</li>
<li>SSL, in it’s full implementation takes 9 separate steps to initiate a single request. When your system is doing thousands of requests per second, this becomes a significant impediment to performance</li>
</ul>
		</aside>
	</section>

	<section>
		<h3>Scaling Options</h3>
		<p>Dynamic Re-Configuration Recap</p>
		<ul>
			<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">upstream_conf</span></pre></li>
			<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">server</span></pre> parameters</li>
			<li> Example:
			<pre><code class="linux" data-trim contenteditable>
				curl -D http://server/upstream_conf?upstream=myServers&id=0&weight=5
			</code></pre></li>
		</ul>
		<aside class="notes"></aside>
	</section>

	 	<section>
		<h3>Sample App</h3>
		<center><img src="assets/images/mraExample.png" style="border:0; background:none; width:70%"></center>
		<aside class="notes"></aside>
	</section>

	<section>
		<h3>Proxy Model</h3>
		<center><img src="assets/images/proxyModel.png" style="border:0; background:none; width:70%"></center>
		<aside class="notes">
<p>This model focuses entirely on in-bound traffic and ignores the whole inter process communication problem. Basically think of it as putting NGINX on a publc agent and letting the services on the private agents fend for themselves. 
The good thing is that:</p>
<ul>
<li>You get all the goodness of HTTP traffic management in this system that you normally get with NGINX</li>
<li>SSL termination</li>
<li>Traffic shaping and security</li>
<li>Caching</li>
<li>With NGINX plus you get robust load-balancing and service discovery</li>
</ul>

<p>This model works well for a simple and flat API or a monolith with some basic microservices attached.
For Kubernetes we have an open source Ingress Controller that allows you to easily implement this system using our OSS or commercial version</p>
<p>NGINX + gives you dynamic upstreams, active health checks, and robust monitoring 
WAF</p>

		</aside>
	</section>

	<section>
		<h3>Router Mesh</h3>
		<center><img src="assets/images/routerMesh.png" style="border:0; background:none; width:70%"></center>
		<aside class="notes">
<p>The next model is called the router mesh, like the proxy model, it has NGINX running in front of the system to manage in bound traffic and gives you all of the benefits of the proxy model</p>

<p>Where it differs is in the introduction of a centralized load balancer between the services. When services need to communicate with other services, they route through the this centralized load balancer and the traffic is distributed to other instances</p>

<p>The Dies Router with NGINX/NGINX Plus work in this manner</p>
<p>Service discovery through DNS and monitoring the service event stream in the registry, but the disadvantage her is it exacerbates the performance problem by adding another hop in the network connection thus requiring another SSL handshake to make it work</p>
<p>So instead of a 9 step SSL handshake, you need to do an 18 step SSL handshake</p>
		</aside>
	</section>

<section>
	<section>
		<h3>Fabric Model</h3>
		<center><img src="assets/images/fabricModel.png" style="border:0; background:none; width:70%"></center>
		<aside class="notes">
<p>The final model is what we call the fabric model</p>
<p>Like the other two models you have a public proxy in front of the system to handle incoming HTTP traffic, where it differs from other models is that:</p>
<ul>
<li>Instead of a centralized router, each container has an instance of NGINX Plus running in the container</li> 
<li>This system acts as a local reverse and forward proxy for all http traffic</li>
<li>Using this system you get service discovery, robust load balancing and most importantly, high performance, encrypted networking</li>
</ul>
		</aside>
	</section>

	<section>
		<h3>Normal Process</h3>
		<center><img src="assets/images/normalProcess.png" style="border:0; background:none; width:68%"></center>
		<aside class="notes">
<p>So let’s look why the fabric model is so good, but first looking at the normal process</p>
<p>Let’s say you have two services that need to talk to each other
In this diagram the Investment manager needs to talk to the user manager to get user data</p>

<p>The investment manager will create a new instance of an HTTP client
The client will doa  DNS request to the service registry (Mesos DNS sitting on top of Zookeeper)</p>

<p>It will get an ip address back of the service
It will then go through the 9-step SSL handshake</p>

<p>Once the data is transferred, it will close down the connection and garbage collect the HTTP client
Service discovery is dependent on the the application to be able to query and understand DNS requests – good luck with SRV records</p>

<p>The load-balancing is dependent on the service registry and is typically the dumbest load balancing option, round-robin DNS
Each and every request has to go through the SSL negotiation process – even if you don’t do CA authentication, it is a 4 step process at minimum.</p>

		</aside>
	</section>

	<section>
		<h3>Detail Process</h3>
		<center><img src="assets/images/detailProcess.png" style="border:0; background:none; width:68%"></center>
		<aside class="notes">
<p>So let’s look in detail of the how the Fabric model works between microservices
The first thing you will notice is that NGINX Plus runs in each service and the application code talks locally to NGINX Plus
Because these are localhost network connections, FastCGI or even file socket connections, they don’t need to be encrypted</p>
<p>
You will also notice that NGINX Plus is connecting to the other microservices NGINX Plus instances</p>
<p>
Finally you will notice that NGINX Plus is connecting to service registry to do service discovery
We will go through each of these steps in detail in just a moment</p>

		</aside>
	</section>

	<section>
		<h3>Service Discovery</h3>
		<center><img src="assets/images/serviceDiscovery.png" style="border:0; background:none; width:68%"></center>
		<aside class="notes">
<p>Having NGINX Plus deal with service discovery is beneficial on a bunch of levels:</p>
<ul>
	<li>You use DNS which is a service discovery mechanism that is very clear and well understood by developers</li>
<li>NGINX Plus has an asynchronous resolver that can query the service resolver on a user definable frequency, say every couple of seconds</li>
<li>Because it is asynchronous and non-blocking, request and response processing continues to happen even as service instances are added and subtracted from the load balancing tool</li>
<li>NGINX Plus as of R9 can also resolve SRV records so that you can optimally use and deploy your resources on your cluster instances</li>
</ul>
		</aside>
	</section>

	<section>
		<h3>LB and Persistent SSL</h3>
		<center><img src="assets/images/lbProcess.png" style="border:0; background:none; width:68%"></center>
		<aside class="notes">
<h4>Load Balancing</h4>
<p>When NGINX Plus gets back the list of User Manager instances, it puts them in the load balancing pool to distribute requests</p>
<p>NGINX has a variety of load balancing schemes that are user definable</p>
<ul>
<li>Least time balancing will send data to the fast responding service, whether you choose header response or full data response</li>
<li>If your system has to connect to a monolith or stateful system, we have connection persistence to make sure requests go back to the proper instance</li>
</ul>

<h4>Persistent SSL</h4>
<p>But here is where the real benefit comes in – stateful, persistent connections between microservices</p>
<p>Remember the first diagram and how the service instance goes through the process of:</p>
<ul>
<li>Creating an HTTP client</li>
<li>Negotiating the SSL connection</li>
<li>Making the request</li>
<li>Closing down the connection</li>
</ul>
<p>Here NGINX creates a connection to the other microservices and, using keepalive connection functionality, maintains that connection across application code requests</p>
<p>Essentially, there are mini VPNs that are created from service to service</p>
<p>In our initial testing we have seen a 77% increase in connection speed
</p>
		</aside>
	</section>
</section>

<section>
		<h3>Circuit Breakers</h3>
		<center><img src="assets/images/circuitBreaker.png" style="border:0; background:none; width:70%"></center>
		<aside class="notes">
<p>As an added benefit, you can build the Circuit Breaker pattern into your microservices using NGINX Plus active health checks
You define an active health check for your service that queries a healthcheck end point
You can have a variety of responses that NGINX can evaluate using our regex functionaliy
If the system is marked as unhealthy, we will throttle back traffic to that instance until it has time to recover
We even go beyond Martin Fowler’s circuit breaker pattern in providing alternate solutions for a variety of circumstances
500 error response
Backup server options
You can even add a slow start feature
</p>
		</aside>
	</section>

<section>
	<h3>Network Considerations</h3>
	<div style="float:left;width:45%;" class="centered">
		<ol>
		  <li>Docker Best Practices</li>
		  <li>Process Failure means Container Failure</li>
		  <li>Adding Another Layer to the Stack</li>
		  <li>Dev Team Have Too Much Power</li>
		  <li>Tooling to Make the Fabric Model, Simple to Create and Deploy</li>
		</ol>
	</div>
	<div style="float:right;width:50%;" class="centered">
		<center><img src="assets/images/networkIssues.png" style="border:0; background:none; width:100%"></center>
	</div>
	<aside class="notes"></aside>
</section>

<section data-state="lab">
		  <h3>Take-Home Lab 6: MRA Deployment</h3>
		   <div style="float:left;width:45%;" class="centered">
		 <ol>
		  <li>Get files from GitHub</li>
		  <li>Make the Docker Image</li>
		  <li>Push Docker Image to Docker Hub</li>
		  <li>Tell DC/OS to pull Docker Image from Docker Hub</li>
		  <li>Test the site</li>
		</ol>
	</div>
	 <div style="float:right;width:50%;" class="centered">

		<center><img src="assets/images/mraDeployment.png" style="border:0; background:none; width:100%"></center>
	</div>

		
		  <aside class ="notes">
<ol>
</ol>

</aside>
</section>

    <!--Next Section-->
 <section data-background="rgb(20, 149, 62)">
     <h2>Performance Tuning</h2>
 </section>

 <section>
    <h3>Module Objectives</h3>
       <p>This module enables you to:</p>
          <ul>
    <li>Explore NGINX Tunables</li>
    <li>Explore Kernel Tunables</li>
    <li>Outline Benchmark Testing</li>
		  </ul>
    <aside class="notes"></aside>
    </section>

    <section>
		<h3>Diagnostic Tools</h3>
		<ul>
			<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">netstat</span></pre> and <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">ss</span></pre> for network issues</li>
			<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">top</span></pre> and <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">vmstat</span></pre> for CPU and RAM issues</li>
			<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">iostat</span></pre> for disc I/O</li>
			<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">tail</span></pre> and <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">curl</span></pre> for general testing/troubleshooting</li>
		</ul>
		<aside class="notes">
<p>These are some important tools and counters that will be the first line of defense in diagnosing performance issues
These will be able to test for things like average load, worker connection and process distribution, idle processes, disc latency, etc.</p>
		</aside>
	</section>


	<section>
		<h3>Configuration Pitfalls</h3>
		<div style="float:left;width:50%;" class="centered">
		<ul>
			<li>Redundant directives</li>
			<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">rewrite</span></pre> vs. <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">return</span></pre></li>
			<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">if</span></pre> is f*@!ing evil!</li>
			<li>Proxying everything</li>
			<li>Hostname issues</li>
		</ul>
		<p></p>
		<div style="text-align:center;"><small>Common Mistakes<a href="https://www.nginx.com/resources/wiki/start/topics/tutorials/config_pitfalls/" target="_blank">...read it....ALL OF IT!</a></small></div>
	</div>
	<div style="float:right;width:45%;" class="centered">
		<center><img src="assets/images/commonPitfalls.png" style="border:0; background:none; width:80%"></center>
	</div>
	<aside class="notes">
<ul>
	<li>Redundant index and root directives</li>
<li>Remember if is evil</li>
<li>Passing uncontrolled requests through gateways (PHP)</li>
<li>Unnecessary use of rewrite directive</li>
<li>Proxying everything (not using try_files )</li>
<li>Hostname resolver issues (i.e. hostname on listen)</li>


</ul>
	</aside>
</section>

	<section>
		<h3>Benchmark Tests</h3>

		<div style="float:left;width:50%;" class="centered">
		<ul>
			<li>Know Testing Metric</li>
			<li>Track each change</li>
			<li>Validate by re-testing</li>
			<li>No change? Revert!</li>
			<li>Test Scnearios
				<ul>
					<li>Soft (static)</li>
					<li>Hard (dynamic)</l>
					</ul></li>
		</ul>
	</div>
	<div style="float:right;width:45%;" class="centered">
		<center><img src="assets/images/badBenchmark.png" style="border:0; background:none; width:80%"></center>
	</div>
		<aside class="notes">
<ul><li>Set a good benchmark: Know what you’re testing or trying to improve (most testing is reactive).</li>
<li>Track and document each change.</li>
<li>Validate findings by re-testing.</li>
<li>Change one thing at a time.</li>
<li>Test multiple scenarios and files:</li>
<li>soft (static file e.g. image).</li>
<li>heavy (dynamic file e.g. PHP).</li>
<li>If no significant change...revert.</li>
</ul>

		</aside>
	</section>


		<section>
			<h3>Kernel Tunables</h3>
		<div style="float:left;width:50%;" class="centered">
		<ul>
			<li>Varies by System</li>
			<li>Backup Defaults</li>
			<li>Tune for Scale</li>
			<li>When in doubt, enlist the help of NGINX Professional Services</li>
			</ul>
	</div>
	<div style="float:right;width:45%;" class="centered">
		<center><img src="assets/images/kernelTuning.png" style="border:0; background:none; width:100%"></center>
	</div>
		<aside class="notes">
<ul><li>Kernel parameters vary from system to system</li>
<li>Backup your defaults via commenting and/or snapshots (stage env)</li>
<li>Aim for scalability because performance goes hand in hand</li>
<li>When in doubt, consult your documentation and NGINX technical support</li>
</ul>
		</aside>
	</section>

		<section>
			<h3>Check Logs</h3>
		<div style="float:left;width:45%;" class="centered">
			<p>Keep Checking Messages</p>
				<ul>
					<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;"># dmesg -c</span></pre></li>
					<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;"># tail -f</span></pre><ul>

					<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">/var/log/kern.log</span></pre></li>
					<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">/var/log/nginx/</span></pre></ul></li>
				</ul></li>
	</div>
	<div style="float:right;width:45%;" class="centered">
		<center><img src="assets/images/Ed.gif" style="border:0; background:none; width:80%"></center>
	</div>
		<aside class="notes">
		</aside>
	</section>

	<section>
		<h3>Ephemeral Ports</h3>
		<ul>
			<li>Echo modified range in /proc file system:
				<pre><code class="linux" data-trim contenteditable>
			 echo “&#60;min&#62; &#60;max&#62; /proc/sys/net/ipv4/ip_local_port_range
			</code></pre>
			</li>
<li>OR append /etc/sysctl.conf (permanent solution) by adding the line:
	<pre><code class="linux" data-trim contenteditable>
	net.ipv4.ip_local_port_range = “ &#60;min value&#62; &#60;max value&#62;”
</code></pre>
</li>
<li>Decrease the active/inactive timeout using:
	<pre><code class="linux" data-trim contenteditable>
	 net.ipv4.tcp_fin_timeout
</code></pre>
</li>
</ul>

		<aside class="notes">
<p>These are the commands to adjust the ephemeral ports, you can append them to system control or using the echo command via the proc file system. The system control method will survive reboots
</p>
<h4>Explanation</h4>
<p>Default TCP port range for outgoing connections is around 32,786-65,536 ports.
Modify the Linux kernel parameters to increase/decrease local available ports. 
Ephemeral port exhaustion, system will run out of port numbers
NGINX Plus by nature is subject to ephemeral port exhaustion and the problems it causes. When NGINX Plus proxies a request to an upstream server, it is the client in the socket-creation process. The default behavior is to bind the socket for the proxied request automatically to a local IP and any ephemeral port available on the host where it is running. If the connection rate is high, i.e. established sockets are moving to waiting state faster than existing sockets are closing, then eventually the available ports are exhausted and new sockets cannot be created.
Configuring keepalives will allow your connections to be more efficient and reduce overhead thus in turn reducing the chance of emphermal port exhaustion
</p>

		</aside>
	</section>

	<section>
		<h3>File Descriptor Limits</h3>
		<p>System Wide vs. User Limit</p>
		<ul>
			<li>View OS limit
				<pre><code class="linux" data-trim contenteditable>
			 # cat /proc/sys/fs/file-max
			</code></pre>
			</li>
<li>Append to sysctl, and restart
	<pre><code class="linux" data-trim contenteditable>
		# fs.file-max = &#60;some value&#62;
# sysctl -p
</code></pre>
</li>
			<li>Adjust User limit on the fly 
				<pre><code class="linux" data-trim contenteditable>
	# ulimit -n
			</code></pre>
			</li>
<li>Typical user limit example:
	<pre><code class="linux" data-trim contenteditable>
	# ulimit -Sc unlimited; nginx
</code></pre>
</li>
		<aside class="notes">
			<p>OS level</p>
			<ul>
				<li>use the cat command to see your system’s limits (/proc/sys/fs/file-max)</li>
<li>Set the max number of file-handles that the kernel can manage on a system level.</li>
<li>generally tune this file to improve the number of open files by increasing the value of /proc/sys/fs/file-max to something reasonable like 256 for every 4M of RAM we have: i.e. for a machine with 128 MB of RAM, set it to 8192 - 128/4=32 32*256=8192.</li>
</ul>

<p>Ulmit</p>
<ul>
	<li>hard limits can only be raised by root (but any process can lower it). Therefore a non-root process cannot overstep a hard limit. This is good for security.</li>
<li>A soft limit can be changed by the process at any time, which is convenient. This is not good for security.</li>
<li>However, a typical use case for increasing soft limits on the fly is to disable core dumps (ulimit -Sc 0) while keeping the option of enabling them for a specific process you're debugging ((ulimit -Sc unlimited; myprocess)).
So in the case of NGiNX it would looks something like this ((ulimit -Sc unlimited; nginx))</li>
</ul>


		</aside>
	</section>

	<section>
		<h3>Backlog Queue</h3>
		<ul>
		<li>Kernel maintains 2 queues for a given listening socket:
			<ul>
	<li>Incomplete connection queue (SYN/received)</li>
	<li>Complete connection queue (ACK/established)</li>
</ul></li>
</ul>
<pre><code class="linux" data-trim contenteditable>
	# net.core.somaxconn 
# net.core.netdev_max_backlog
</code></pre>

		<aside class="notes">
To understand the backlog argument, we must realize that for a given listening socket, the kernel maintains two queues: incomplete queue (contains an entry for each SYN that has arrived from a client for which the server is awaiting completion of the TCP three-way handshake. These sockets are in the SYN_RCVD state) and the completed queue (contains an entry for each client with whom the TCP three-way handshake has completed. These sockets are in the ESTABLISHED state).
Default backlog number is very low, and this is often acceptable because NGINX typically serves content very quickly. However if you’re having a high amount of incoming traffic and your connections are stalling you can tune the settings in the following slides to help.
Use net.core.somaxconn to increase maximum number of incoming connections—default is 128.
Example: #net.core.somaxconn = 1024
Use net.core.netdev_max_backlog to increase the rate the network card buffers packets before offloading to CPU
Example: # net.core.somax.conn = 65536

		</aside>
	</section>
</section>

<section>
	<section>
		<h3>NGINX Tunables</h3>
		<ul>
			<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">worker_rlimit_nofile</span></pre>: adjusts max number of FD each <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">worker_process</span></pre> can use</li>
			<li>TCP Transport tunables: <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">sendfile</span></pre>, <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">tcp_nodelay</span></pre>, <pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">tcp_cork</span></pre> (<pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">tcp_nopush</span></pre>)</li>
			<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">keepalive</span></pre></li>
		</ul>
		<aside class="notes"></aside>
	</section>

	<section>
		<h3><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 60px;">keepalives</span></pre></h3>
		<aside class="notes"></aside>
		<h4>Client Keepalives</h4>
		<center><img src="assets/images/clientKeepalives.png" style="border:0; background:none; width:79%"></center>
		<h4>Upstream Keepalives</h4>
		<pre><code class="linux" data-trim contenteditable>
		upstream backend {
    server localhost;
    keepalive 32;
}
server {
    location / {
        proxy_pass http://backend;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
    }
}
</code></pre>

	</section>

	<section>
		<h3>Socket Sharding</h3>
		<div>
  <div style="float:left;width:50%;" class="centered"><center><img class="fragment" data-fragment-index="0" src="assets/images/lockContention.png" style="border:0; background:none; width:93%; position:relative;"></center>
  <center><p class="fragment" data-fragment-index="0">
  	Lock Contention</p></center>
  </div>
 
  <div style="float:right;width:50%;" class="centered"><center><img class="fragment" data-fragment-index="1" src="assets/images/socketSharding.png" style="border:0; background:none; width:100; position:relative;"></center>
  	<pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;"><center><p class="fragment fade-in" data-fragment-index="1">reuseport</p></center></span></pre>
</div>
		<aside class="notes"></aside>
	</section>

	<section>
		<h3>Worker Tuning</h3>
		<ul>
			<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">worker_processes</span></pre>: Set how many workers to deploy</li>
			<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">worker_cpu_affinity</span></pre>: Binds worker to a CPU core</li>
			<li><pre style="display:inline; color:rgb(240,168,40);"><span style="font-size: 30px;">worker_connections</span></pre>: Set max conn to each worker</li>
		</ul>
		<aside class="notes"></aside>
	</section>

	<section>
		<h3>Buffers and Timeouts</h3>
		<div style="float:left;width:50%;" class="centered">
		<ul>
			<li>Throughput optimization</li>
			<li>Speed up client upload/download</li>
			<li>Can free up backends</li>
			<li>Important for caching</li>
			</ul>
	</div>
	<div style="float:right;width:50%;" class="centered">
		<center><img src="assets/images/bufferTimeout.png" style="border:0; background:none; width:100%"></center>
	</div>
		<aside class="notes"></aside>
	</section>
</section>
                 <!--Next Section-->
		<section data-background="rgb(20, 149, 62)">
                  <h2>Additional Resources</h2>
                 </section>

		<section>
                  <h3>Further Information</h3>
                  <li><a href="https://nginx.org/en/docs/" target="_blank">NGINX Documentation</a></li>
                  <li><a href="https://www.nginx.com/resources/admin-guide/" target="_blank">NGINX Admin Guides</a></li>
                  <li><a href="https://www.nginx.com/blog/" target="_blank">NGINX Blog</a></li>
                  <aside class="notes"></aside>
                 </section>

                 <section>
                   <h3>Q&A</h3>
                   <li><a href="http://www.surveygizmo.com/s3/3505652/NGINX-Advanced-Survey" target="_blank">Survey!</a></li>
                   <li>Sales: <a href="mailto:nginx-inquiries@nginx.com" target="_top">nginx-inquiries@nginx.com</a></li>
                   <aside class="notes"></aside>
                 </section>
		</div>
	  </div>
	  
      <script src="lib/js/jquery-2.2.4.min.js"></script>
      <script src="lib/js/head.min.js"></script>
	  <script src="js/reveal.js"></script>

	  <script>

			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
            //width: 1200,


            controls: true,
				progress: true,
				history: true,
				center: true,
                slideNumber: true,
				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
          			{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
                    { src: 'lib/js/jquery-2.2.4.min.js'},
                    { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
                    { src: 'plugin/external/external.js', condition: function() { return !!document.querySelector( '[data-external]' ); } },
                    { src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

            Reveal.addEventListener( 'slidechanged', function( event ) {
//            console.log(event.currentSlide.getAttribute("data-state"))
// if we're on a lab slide, unhide the lab image, otherwise hide it.


            if(event.currentSlide.getAttribute("data-state") === "lab"){
                //document.getElementById("lab_pic").style.visibility="visible";


            if(document.getElementById("lab_pic").style.visibility=="visible"){
                document.getElementById("lab_pic").style.visibility="visible";
            }else{
      $("#lab_pic").css({opacity: 0.0, visibility: "visible"}).animate({opacity: 1}, 200);
            }

            }else{
               //(document.getElementById("lab_pic").style.visibility=="hidden";
               $("#lab_pic").css({opacity: 1.0, visibility: "hidden"}).animate({opacity: 0}, 200);
            }

            } );

		</script>

	</body>
</html>
